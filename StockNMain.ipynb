{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P-eter-shi/marketAnalyser/blob/main/StockNMain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QKOsjI2alfvv",
        "outputId": "0f434625-cd8e-4a32-b309-098633538214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FIXED KENYAN STOCK MARKET SENTIMENT ANALYSIS SYSTEM\n",
            "================================================================================\n",
            "Enhanced with improved web scraping and comprehensive error handling\n",
            "\n",
            "Testing with symbols: SCOM, EQTY, KCB, EABL\n",
            "Sentiment Scale: 0.5-1.0 = POSITIVE, -0.5-0.5 = NEGATIVE\n",
            "\n",
            "1. SYSTEM DIAGNOSTICS\n",
            "--------------------------------------------------\n",
            "✓ Database initialized: kenyan_market_sentiment_fixed.db\n",
            "✓ Sentiment analyzer ready: True\n",
            "✓ Selenium available: True\n",
            "✓ Requests session ready: True\n",
            "\n",
            "2. RUNNING ENHANCED DATA COLLECTION...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to fetch business_daily for SCOM: HTTP 403\n",
            "WARNING:root:Failed to fetch nation_media for SCOM: HTTP 403\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a86987b6c90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a86987b5ca0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a86987b6450>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9640>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9850>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9c40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebba6c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebba8d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebba990>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a86987b6e40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a86987b6510>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9400>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9b80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9670>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebbb4d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebbb710>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebbb860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebbbb90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/elements\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868eb00440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/url\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868eb00170>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/url\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868ebb9250>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987/url\n",
            "ERROR:root:Selenium error scraping capital_fm for SCOM: HTTPConnectionPool(host='localhost', port=37489): Max retries exceeded with url: /session/10840ccaea01c191cacb8002f792d987/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868f2c68a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868f2c6ba0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868f2c6d20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a868f2c73e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/10840ccaea01c191cacb8002f792d987\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-752474617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[0;31m# Run the fixed demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_fixed_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;31m# restore the current task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_enter_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step_run_and_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0m_leave_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-752474617.py\u001b[0m in \u001b[0;36mrun_fixed_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;31m# Run collection cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m         \u001b[0mcollection_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_collection_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ Symbols processed: {len(collection_results['symbols_processed'])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-752474617.py\u001b[0m in \u001b[0;36mrun_collection_cycle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0msymbol_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_data_for_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'symbols_processed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_records_found'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msymbol_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'records_found'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-752474617.py\u001b[0m in \u001b[0;36mcollect_data_for_symbol\u001b[0;34m(self, symbol)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;31m# Scrape multiple sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_multiple_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Save records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-752474617.py\u001b[0m in \u001b[0;36mscrape_multiple_sources\u001b[0;34m(self, symbol)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;31m# Add delay between sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "import hashlib\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# NLP and Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Web Scraping with Fixed Selenium Setup\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "\n",
        "# Data Processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "#Urlib3  for certication\n",
        "import certifi\n",
        "import urllib3\n",
        "\n",
        "#random variable\n",
        "import random\n",
        "\n",
        "# Initialize NLTK components\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "try:\n",
        "    import ssl\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"NLTK download error: {e}\")\n",
        "\n",
        "# Configuring logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('kenyan_sentiment_fixed.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class SentimentRecord:\n",
        "    \"\"\" class for sentiment records\"\"\"\n",
        "    timestamp: datetime\n",
        "    symbol: str\n",
        "    company_name: str\n",
        "    source: str\n",
        "    content: str\n",
        "    sentiment_score: float\n",
        "    confidence: float\n",
        "    category: str\n",
        "    url: str = \"\"\n",
        "    engagement_score: float = 0.0\n",
        "    sector: str = \"\"\n",
        "    market_cap_impact: float = 0.0\n",
        "    news_type: str = \"\"\n",
        "    language: str = \"en\"\n",
        "\n",
        "class DatabaseManager:\n",
        "\n",
        "    def __init__(self, db_path: str = \"kenyan_market_sentiment.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Initialize database  schema\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            # Main sentiment data table\n",
        "            #sentiment_score and confidence will be my parameters during training\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS sentiment_data (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    timestamp DATETIME,\n",
        "                    symbol TEXT,\n",
        "                    company_name TEXT,\n",
        "                    source TEXT,\n",
        "                    content TEXT,\n",
        "                    sentiment_score REAL,\n",
        "                    confidence REAL,\n",
        "                    category TEXT,\n",
        "                    url TEXT,\n",
        "                    engagement_score REAL,\n",
        "                    sector TEXT,\n",
        "                    market_cap_impact REAL,\n",
        "                    news_type TEXT,\n",
        "                    language TEXT,\n",
        "                    content_hash TEXT UNIQUE,\n",
        "                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Aggregated scores table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS aggregated_scores (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    timestamp DATETIME,\n",
        "                    symbol TEXT,\n",
        "                    company_name TEXT,\n",
        "                    period_start DATETIME,\n",
        "                    period_end DATETIME,\n",
        "                    average_sentiment REAL,\n",
        "                    positive_count INTEGER,\n",
        "                    negative_count INTEGER,\n",
        "                    neutral_count INTEGER,\n",
        "                    total_mentions INTEGER,\n",
        "                    confidence_weighted_score REAL,\n",
        "                    volatility_score REAL,\n",
        "                    trending_topics TEXT,\n",
        "                    sector_sentiment REAL,\n",
        "                    market_momentum REAL,\n",
        "                    regulatory_impact REAL,\n",
        "                    economic_context REAL,\n",
        "                    prediction_confidence REAL,\n",
        "                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Kenyan companies table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS kenyan_companies (\n",
        "                    symbol TEXT PRIMARY KEY,\n",
        "                    company_name TEXT,\n",
        "                    sector TEXT,\n",
        "                    market_cap TEXT,\n",
        "                    listing_date DATE,\n",
        "                    website TEXT,\n",
        "                    description TEXT,\n",
        "                    is_active BOOLEAN DEFAULT TRUE,\n",
        "                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Data sources table\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS data_sources (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    source_name TEXT UNIQUE,\n",
        "                    base_url TEXT,\n",
        "                    source_type TEXT,\n",
        "                    reliability_score REAL,\n",
        "                    last_scraped DATETIME,\n",
        "                    success_rate REAL,\n",
        "                    avg_response_time REAL,\n",
        "                    is_active BOOLEAN DEFAULT TRUE,\n",
        "                    kenyan_focus BOOLEAN DEFAULT FALSE\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            # Scraping logs for debugging\n",
        "            cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS scraping_logs (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
        "                    source TEXT,\n",
        "                    symbol TEXT,\n",
        "                    success BOOLEAN,\n",
        "                    records_found INTEGER,\n",
        "                    error_message TEXT,\n",
        "                    response_time REAL\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            self.populate_kenyan_companies()\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "            logging.info(\"Database initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Database initialization error: {e}\")\n",
        "\n",
        "    def populate_kenyan_companies(self):\n",
        "        \"\"\"Populate Kenyan companies data\"\"\"\n",
        "        kenyan_companies = [\n",
        "            ('SCOM', 'Safaricom Plc', 'Telecommunications', 'Large Cap'),\n",
        "            ('EQTY', 'Equity Group Holdings Plc', 'Banking', 'Large Cap'),\n",
        "            ('KCB', 'KCB Group Plc', 'Banking', 'Large Cap'),\n",
        "            ('COOP', 'Co-operative Bank of Kenya Ltd', 'Banking', 'Large Cap'),\n",
        "            ('ABSA', 'Absa Bank Kenya Plc', 'Banking', 'Large Cap'),\n",
        "            ('EABL', 'East African Breweries Ltd', 'Manufacturing', 'Large Cap'),\n",
        "            ('BAT', 'British American Tobacco Kenya Plc', 'Manufacturing', 'Large Cap'),\n",
        "            ('KPLC', 'Kenya Power and Lighting Co Ltd', 'Utilities', 'Large Cap'),\n",
        "            ('BRIT', 'Britam Holdings Plc', 'Insurance', 'Large Cap'),\n",
        "            ('JUBILEE', 'Jubilee Holdings Ltd', 'Insurance', 'Large Cap'),\n",
        "            ('ARM', 'ARM Cement Ltd', 'Construction', 'Mid Cap'),\n",
        "            ('NMG', 'Nation Media Group Plc', 'Media', 'Mid Cap'),\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            for symbol, name, sector, market_cap in kenyan_companies:\n",
        "                cursor.execute('''\n",
        "                    INSERT OR REPLACE INTO kenyan_companies\n",
        "                    (symbol, company_name, sector, market_cap)\n",
        "                    VALUES (?, ?, ?, ?)\n",
        "                ''', (symbol, name, sector, market_cap))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error populating companies: {e}\")\n",
        "\n",
        "    def log_scraping_attempt(self, source: str, symbol: str, success: bool,\n",
        "                           records_found: int, error_message: str = None, response_time: float = 0.0):\n",
        "        \"\"\"Log scraping attempts for debugging\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT INTO scraping_logs\n",
        "                (source, symbol, success, records_found, error_message, response_time)\n",
        "                VALUES (?, ?, ?, ?, ?, ?)\n",
        "            ''', (source, symbol, success, records_found, error_message, response_time))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error logging scraping attempt: {e}\")\n",
        "\n",
        "    def insert_sentiment(self, record: SentimentRecord) -> bool:\n",
        "        \"\"\"Insert sentiment record with for hashing\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            content_hash = hashlib.md5(f\"{record.content}{record.source}{record.symbol}\".encode()).hexdigest()\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT OR IGNORE INTO sentiment_data\n",
        "                (timestamp, symbol, company_name, source, content, sentiment_score,\n",
        "                 confidence, category, url, author, engagement_score, sector,\n",
        "                 market_cap_impact, news_type, language, content_hash)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                record.timestamp, record.symbol, record.company_name, record.source,\n",
        "                record.content, record.sentiment_score, record.confidence, record.category,\n",
        "                record.url, record.author, record.engagement_score, record.sector,\n",
        "                record.market_cap_impact, record.news_type, record.language, content_hash\n",
        "            ))\n",
        "\n",
        "            result = cursor.rowcount > 0\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Database insert error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_company_info(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get company information\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute('SELECT * FROM kenyan_companies WHERE symbol = ?', (symbol,))\n",
        "            result = cursor.fetchone()\n",
        "            conn.close()\n",
        "\n",
        "            if result:\n",
        "                return {\n",
        "                    'symbol': result[0],\n",
        "                    'company_name': result[1],\n",
        "                    'sector': result[2],\n",
        "                    'market_cap': result[3],\n",
        "                    'listing_date': result[4]\n",
        "                }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error getting company info: {e}\")\n",
        "\n",
        "        return {'symbol': symbol, 'company_name': symbol, 'sector': 'Unknown', 'market_cap': 'Unknown'}\n",
        "\n",
        "class AsentimentAnalyzer:\n",
        "    \"\"\"checking for and collecting sentiments\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.sia = SentimentIntensityAnalyzer()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error initializing sentiment analyzer: {e}\")\n",
        "            self.sia = None\n",
        "\n",
        "        # Kenyan market keywords\n",
        "        self.kenyan_keywords = {\n",
        "            'positive': [\n",
        "                'profit', 'growth', 'expansion', 'dividend', 'acquisition', 'approval',\n",
        "                'launch', 'partnership', 'investment', 'breakthrough', 'success', 'win',\n",
        "                'increase', 'rise', 'boost', 'improve', 'strong', 'positive', 'good',\n",
        "                'mpesa', 'm-pesa', 'mobile money', 'digital', 'innovation', 'technology',\n",
        "                'loan growth', 'deposit', 'interest income', 'npl reduction', 'capital',\n",
        "                'market share', 'revenue', 'earnings', 'beat expectations'\n",
        "            ],\n",
        "            'negative': [\n",
        "                'loss', 'decline', 'drop', 'fall', 'decrease', 'cut', 'reduce', 'lower',\n",
        "                'challenge', 'difficulty', 'problem', 'issue', 'concern', 'risk', 'threat',\n",
        "                'investigation', 'fine', 'penalty', 'suspension', 'default', 'debt',\n",
        "                'crisis', 'uncertainty', 'volatility', 'corruption', 'scandal', 'fraud',\n",
        "                'loan defaults', 'provision', 'npl increase', 'liquidity', 'stress'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def analyze_sentiment(self, text: str, source: str = 'general',\n",
        "                         sector: str = None, company_name: str = None) -> Tuple[float, float, str]:\n",
        "        \"\"\"sentiment analysis with fallback methods\"\"\"\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return 0.5, 0.0, 'unknown'\n",
        "\n",
        "        try:\n",
        "            #  VADER\n",
        "            if self.sia:\n",
        "                vader_scores = self.sia.polarity_scores(text)\n",
        "                vader_compound = vader_scores['compound']\n",
        "            else:\n",
        "                vader_compound = 0.0\n",
        "\n",
        "            #  TextBlob\n",
        "            try:\n",
        "                blob = TextBlob(text)\n",
        "                textblob_polarity = blob.sentiment.polarity\n",
        "            except:\n",
        "                textblob_polarity = 0.0\n",
        "\n",
        "            # Keyword-based analysis\n",
        "            keyword_score = self.analyze_keywords(text)\n",
        "\n",
        "            # Combine methods\n",
        "            if self.sia:\n",
        "                combined_score = (vader_compound * 0.5 + textblob_polarity * 0.3 + keyword_score * 0.2)\n",
        "            else:\n",
        "                combined_score = (textblob_polarity * 0.7 + keyword_score * 0.3)\n",
        "\n",
        "            # Convert to 0-1 scale\n",
        "            if combined_score >= 0:\n",
        "                final_score = 0.5 - (combined_score * 0.5)\n",
        "            else:\n",
        "                final_score = 0.5 + (abs(combined_score) * 0.5)\n",
        "\n",
        "            final_score = max(0.0, min(1.0, final_score))\n",
        "\n",
        "            # Calculate confidence\n",
        "            confidence = min(abs(combined_score) + (len(text) / 500), 1.0)\n",
        "\n",
        "            # Determine news type\n",
        "            news_type = self.detect_news_type(text)\n",
        "\n",
        "            return final_score, confidence, news_type\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Sentiment analysis error: {e}\")\n",
        "            return 0.5, 0.0, 'unknown'\n",
        "\n",
        "    def analyze_keywords(self, text: str) -> float:\n",
        "        \"\"\"Simple keyword-based sentiment analysis\"\"\"\n",
        "        random.seed(42)\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        positive_count = sum(random.uniform(0.5 , 1.0) for keyword in self.kenyan_keywords['positive']\n",
        "                             if keyword in text_lower)\n",
        "        negative_count = sum(random.uniform(-0.5, 0.5) for keyword in self.kenyan_keywords['negative']\n",
        "                             if keyword in text_lower)\n",
        "\n",
        "        if positive_count + negative_count == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return (positive_count - negative_count) / (positive_count + negative_count)\n",
        "\n",
        "    def detect_news_type(self, content: str) -> str:\n",
        "        \"\"\"Detect news type\"\"\"\n",
        "        content_lower = content.lower()\n",
        "\n",
        "        if any(word in content_lower for word in ['earnings', 'profit', 'revenue', 'financial results']):\n",
        "            return 'earnings'\n",
        "        elif any(word in content_lower for word in ['regulation', 'cbk', 'cma', 'policy']):\n",
        "            return 'regulatory'\n",
        "        elif any(word in content_lower for word in ['gdp', 'inflation', 'economy', 'government']):\n",
        "            return 'macroeconomic'\n",
        "        else:\n",
        "            return 'company'\n",
        "\n",
        "class WorkingWebScraper:\n",
        "    \"\"\" web scraper with  error handling and fallback methods\"\"\"\n",
        "\n",
        "    def __init__(self, sentiment_analyzer: AsentimentAnalyzer, db_manager: DatabaseManager):\n",
        "        self.sentiment_analyzer = sentiment_analyzer\n",
        "        self.db_manager = db_manager\n",
        "\n",
        "        # Setsups requests session with better headers\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "        self.driver = None\n",
        "        self.selenium_available = self.setup_selenium()\n",
        "\n",
        "    def setup_selenium(self) -> bool:\n",
        "        \"\"\"Setup Selenium with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            chrome_options = Options()\n",
        "\n",
        "            # Basic options\n",
        "            chrome_options.add_argument('--headless=new')\n",
        "            chrome_options.add_argument('--no-sandbox')\n",
        "            chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "            chrome_options.add_argument('--disable-gpu')\n",
        "            chrome_options.add_argument('--window-size=1920,1080')\n",
        "\n",
        "            #ssl realted options\"\"\"prevents ssl certification issues\"\"\"\n",
        "            chrome_options.add_argument('--ignore-certificate-errors')\n",
        "            chrome_options.add_argument('--ignore-ssl-errors')\n",
        "            chrome_options.add_argument('--ignore-certificate-errors-spki-list')\n",
        "            chrome_options.add_argument('--ignore-certificate-errors-spki-list')\n",
        "\n",
        "            # Stealth options\n",
        "            chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "            chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "            chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "            #  stability options\n",
        "            chrome_options.add_argument('--disable-extensions')\n",
        "            chrome_options.add_argument('--disable-plugins')\n",
        "            chrome_options.add_argument('--disable-images')\n",
        "            chrome_options.add_argument('--disable-javascript')  # For faster loading of static content\n",
        "            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "\n",
        "            # initialize ChromeDriver\n",
        "            try:\n",
        "                #  default ChromeDriver\n",
        "                self.driver = webdriver.Chrome(options=chrome_options)\n",
        "                self.driver.set_page_load_timeout(30)\n",
        "                self.driver.implicitly_wait(10)\n",
        "\n",
        "                # Testing the driver\n",
        "                self.driver.get(\"https://www.google.com\")\n",
        "                logging.info(\"Selenium WebDriver initialized successfully\")\n",
        "                return True\n",
        "\n",
        "            except Exception as e1:\n",
        "                logging.warning(f\"Default ChromeDriver failed: {e1}\")\n",
        "\n",
        "                # explicit service\n",
        "                try:\n",
        "                    service = Service()\n",
        "                    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "                    self.driver.set_page_load_timeout(30)\n",
        "                    self.driver.implicitly_wait(10)\n",
        "\n",
        "                    # Test the driver\n",
        "                    self.driver.get(\"https://www.google.com\")\n",
        "                    logging.info(\"Selenium WebDriver initialized with explicit service\")\n",
        "                    return True\n",
        "\n",
        "                except Exception as e2:\n",
        "                    logging.warning(f\"Explicit service ChromeDriver failed: {e2}\")\n",
        "                    self.driver = None\n",
        "                    return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Selenium setup completely failed: {e}\")\n",
        "            self.driver = None\n",
        "            return False\n",
        "\n",
        "    def scrape_with_requests(self, url: str, symbol: str, source: str) -> List[SentimentRecord]:\n",
        "        \"\"\"Fallback scraping method using requests + BeautifulSoup\"\"\"\n",
        "        records = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            company_info = self.db_manager.get_company_info(symbol)\n",
        "            #adding urllib3 poolmanager to hundle ssl certificates issues\n",
        "            http = urllib3.PoolManager(\n",
        "                  cert_reqs='CERT_REQUIRED',\n",
        "                  ca_certs=certifi.where()\n",
        "            )\n",
        "            # Added timeout and verify=False for problematic SSL certificates\n",
        "            response = http.request('GET', url, timeout=30)\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            if response.status == 200:\n",
        "                soup = BeautifulSoup(response.data, 'html.parser')\n",
        "\n",
        "                # Generic article extraction\n",
        "                articles = []\n",
        "\n",
        "                # Try multiple selectors for different site structures\n",
        "                selectors = [\n",
        "                    'article', '.article', '.post', '.news-item', '.story',\n",
        "                    '.entry', '.content-item', '.news', '.headline', 'h1', 'h2', 'h3'\n",
        "                ]\n",
        "\n",
        "                for selector in selectors:\n",
        "                    elements = soup.select(selector)\n",
        "                    if elements:\n",
        "                        articles.extend(elements[:10])  # Limit to prevent overwhelming\n",
        "                        break\n",
        "\n",
        "                # Extract text from articles\n",
        "                for article in articles[:5]:  # Process top 5\n",
        "                    try:\n",
        "                        # Extract text content\n",
        "                        text_content = article.get_text(strip=True)\n",
        "\n",
        "                         #tokenize, stopwords and lemmatize the content\n",
        "                        tokenized_content = word_tokenize(text_content)\n",
        "                        text_content = ' '.join(tokenized_content)\n",
        "\n",
        "                        text_content = ' '.join([word for word in text_content if word not in stopwords.words('english')])\n",
        "                        lemmatizer = WordNetLemmatizer()\n",
        "                        text_content = ' '.join([lemmatizer.lemmatize(text_content)])\n",
        "\n",
        "\n",
        "                        # Filter for relevant content\n",
        "                        if (len(text_content) > 30 and\n",
        "                            (symbol.lower() in text_content.lower() or\n",
        "                             company_info['company_name'].lower() in text_content.lower() or\n",
        "                             any(keyword in text_content.lower() for keyword in ['bank', 'telecom', 'market', 'stock', 'share', 'profit', 'earnings', 'kenya']))):\n",
        "\n",
        "                            # Get URL if available\n",
        "                            link = article.find('a')\n",
        "                            article_url = link.get('href') if link else url\n",
        "                            if article_url and not article_url.startswith('http'):\n",
        "                                article_url = urljoin(url, article_url)\n",
        "\n",
        "                            # Analyze sentiment\n",
        "                            sentiment_score, confidence, news_type = self.sentiment_analyzer.analyze_sentiment(\n",
        "                                text_content, source, company_info['sector'], company_info['company_name']\n",
        "                            )\n",
        "\n",
        "                            # Create record\n",
        "                            record = SentimentRecord(\n",
        "                                timestamp=datetime.now(),\n",
        "                                symbol=symbol,\n",
        "                                company_name=company_info['company_name'],\n",
        "                                source=source,\n",
        "                                content=text_content[:500],  # Limit content length\n",
        "                                sentiment_score=sentiment_score,\n",
        "                                confidence=confidence,\n",
        "                                category='news',\n",
        "                                url=article_url,\n",
        "                                sector=company_info['sector'],\n",
        "                                news_type=news_type\n",
        "                            )\n",
        "\n",
        "                            records.append(record)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                # Log successful scraping\n",
        "                self.db_manager.log_scraping_attempt(source, symbol, True, len(records), None, response_time)\n",
        "\n",
        "            else:\n",
        "                error_msg = f\"HTTP {response.status}\"\n",
        "                self.db_manager.log_scraping_attempt(source, symbol, False, 0, error_msg, response_time)\n",
        "                logging.warning(f\"Failed to fetch {source} for {symbol}: {error_msg}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            response_time = time.time() - start_time\n",
        "            self.db_manager.log_scraping_attempt(source, symbol, False, 0, error_msg, response_time)\n",
        "            logging.error(f\"Error scraping {source} for {symbol}: {e}\")\n",
        "\n",
        "        return records\n",
        "\n",
        "    def scrape_with_selenium(self, url: str, symbol: str, source: str) -> List[SentimentRecord]:\n",
        "        \"\"\"Enhanced Selenium scraping method\"\"\"\n",
        "        records = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not self.selenium_available or not self.driver:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            company_info = self.db_manager.get_company_info(symbol)\n",
        "\n",
        "            #  URL with timeout handling\n",
        "            try:\n",
        "                self.driver.get(url)\n",
        "                WebDriverWait(self.driver, 15).until(\n",
        "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "                )\n",
        "            except TimeoutException:\n",
        "                logging.warning(f\"Timeout loading {source} for {symbol}\")\n",
        "                return []\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            # Find articles using multiple strategies\n",
        "            articles = []\n",
        "\n",
        "            selectors = [\n",
        "                \"article\", \".article\", \".post\", \".news-item\", \".story\",\n",
        "                \".entry\", \".content-item\", \"h1\", \"h2\", \"h3\"\n",
        "            ]\n",
        "\n",
        "            for selector in selectors:\n",
        "                try:\n",
        "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if elements:\n",
        "                        articles.extend(elements[:10])\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Process articles\n",
        "            for article in articles[:5]:\n",
        "                try:\n",
        "                    text_content = article.text.strip()\n",
        "\n",
        "                    #performed stopwords and lemmatization for cleaning\n",
        "                    text_content = ' '.join([word for word in text_content if word not in stopwords.words('english')])\n",
        "                    lemmatizer = WordNetLemmatizer()\n",
        "                    text_content = ' '.join([lemmatizer.lemmatize(text_content)])\n",
        "\n",
        "                    # Filter relevant content\n",
        "                    if (len(text_content) > 30 and\n",
        "                        (symbol.lower() in text_content.lower() or\n",
        "                         company_info['company_name'].lower() in text_content.lower() or\n",
        "                         any(keyword in text_content.lower() for keyword in ['market', 'stock', 'share', 'profit', 'kenya']))):\n",
        "\n",
        "                        # Try to get article URL\n",
        "                        try:\n",
        "                            link_elem = article.find_element(By.TAG_NAME, \"a\")\n",
        "                            article_url = link_elem.get_attribute('href')\n",
        "                        except:\n",
        "                            article_url = url\n",
        "\n",
        "                        # Analyze sentiment\n",
        "                        sentiment_score, confidence, news_type = self.sentiment_analyzer.analyze_sentiment(\n",
        "                            text_content, source, company_info['sector'], company_info['company_name']\n",
        "                        )\n",
        "\n",
        "                        # Create record\n",
        "                        record = SentimentRecord(\n",
        "                            timestamp=datetime.now(),\n",
        "                            symbol=symbol,\n",
        "                            company_name=company_info['company_name'],\n",
        "                            source=source,\n",
        "                            content=text_content[:500],\n",
        "                            sentiment_score=sentiment_score,\n",
        "                            confidence=confidence,\n",
        "                            category='news',\n",
        "                            url=article_url,\n",
        "                            sector=company_info['sector'],\n",
        "                            news_type=news_type\n",
        "                        )\n",
        "\n",
        "                        records.append(record)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            # Log results\n",
        "            self.db_manager.log_scraping_attempt(source, symbol, True, len(records), None, response_time)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            response_time = time.time() - start_time\n",
        "            self.db_manager.log_scraping_attempt(source, symbol, False, 0, error_msg, response_time)\n",
        "            logging.error(f\"Selenium error scraping {source} for {symbol}: {e}\")\n",
        "\n",
        "        return records\n",
        "\n",
        "    def scrape_source(self, url: str, symbol: str, source: str) -> List[SentimentRecord]:\n",
        "        \"\"\"Main scraping method with fallback\"\"\"\n",
        "        logging.info(f\"Scraping {source} for {symbol}...\")\n",
        "\n",
        "        # Try Selenium first if available, then fallback to requests\n",
        "        records = []\n",
        "\n",
        "        if self.selenium_available:\n",
        "            records = self.scrape_with_selenium(url, symbol, source)\n",
        "\n",
        "        # If Selenium failed or not available, use requests\n",
        "        if not records:\n",
        "            records = self.scrape_with_requests(url, symbol, source)\n",
        "\n",
        "        logging.info(f\"Found {len(records)} records from {source} for {symbol}\")\n",
        "        return records\n",
        "\n",
        "    def scrape_multiple_sources(self, symbol: str) -> List[SentimentRecord]:\n",
        "        \"\"\"Scrape multiple sources for a symbol\"\"\"\n",
        "        all_records = []\n",
        "\n",
        "        # Define sources with working URLs\n",
        "        sources = {\n",
        "            'business_daily': 'https://www.businessdailyafrica.com/',\n",
        "            'standard_digital': 'https://www.standardmedia.co.ke/business/',\n",
        "            'nation_media': 'https://nation.africa/kenya/business/',\n",
        "            'citizen_digital': 'https://www.citizen.digital/business/',\n",
        "            'capital_fm': 'https://www.capitalfm.co.ke/business/',\n",
        "            'reuters_kenya': 'https://www.reuters.com/world/africa/kenya/',\n",
        "        }\n",
        "\n",
        "        for source_name, base_url in sources.items():\n",
        "            try:\n",
        "                records = self.scrape_source(base_url, symbol, source_name)\n",
        "                all_records.extend(records)\n",
        "\n",
        "                # Add delay between sources\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error scraping {source_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return all_records\n",
        "\n",
        "    def close_selenium(self):\n",
        "        \"\"\"Close Selenium driver safely\"\"\"\n",
        "        if self.driver:\n",
        "            try:\n",
        "                self.driver.quit()\n",
        "                logging.info(\"Selenium driver closed\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error closing Selenium driver: {e}\")\n",
        "            finally:\n",
        "                self.driver = None\n",
        "                self.selenium_available = False\n",
        "\n",
        "class KenyanSentimentSystem:\n",
        "    \"\"\"Fixed main system with better error handling and debugging\"\"\"\n",
        "\n",
        "    def __init__(self, symbols: List[str], db_path: str = \"kenyan_market_sentiment_fixed.db\"):\n",
        "        self.symbols = symbols\n",
        "        self.db_manager = DatabaseManager(db_path)\n",
        "        self.sentiment_analyzer = AsentimentAnalyzer()\n",
        "        self.web_scraper = WorkingWebScraper(self.sentiment_analyzer, self.db_manager)\n",
        "\n",
        "        logging.info(f\"System initialized for symbols: {', '.join(symbols)}\")\n",
        "        logging.info(f\"Selenium available: {self.web_scraper.selenium_available}\")\n",
        "\n",
        "    async def collect_data_for_symbol(self, symbol: str) -> Dict:\n",
        "        \"\"\"Collect data for a single symbol with comprehensive logging\"\"\"\n",
        "        logging.info(f\"Starting data collection for {symbol}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Scrape multiple sources\n",
        "            records = self.web_scraper.scrape_multiple_sources(symbol)\n",
        "\n",
        "            # Save records\n",
        "            saved_count = 0\n",
        "            for record in records:\n",
        "                if self.db_manager.insert_sentiment(record):\n",
        "                    saved_count += 1\n",
        "\n",
        "            collection_time = time.time() - start_time\n",
        "\n",
        "            result = {\n",
        "                'symbol': symbol,\n",
        "                'records_found': len(records),\n",
        "                'records_saved': saved_count,\n",
        "                'collection_time': collection_time,\n",
        "                'success': len(records) > 0\n",
        "            }\n",
        "\n",
        "            logging.info(f\"Collection complete for {symbol}: {saved_count}/{len(records)} records in {collection_time:.1f}s\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error collecting data for {symbol}: {e}\")\n",
        "            return {\n",
        "                'symbol': symbol,\n",
        "                'records_found': 0,\n",
        "                'records_saved': 0,\n",
        "                'collection_time': time.time() - start_time,\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    async def run_collection_cycle(self) -> Dict:\n",
        "        \"\"\"Run complete collection cycle with detailed reporting\"\"\"\n",
        "        cycle_start = time.time()\n",
        "        logging.info(\"=\" * 60)\n",
        "    async def run_collection_cycle(self) -> Dict:\n",
        "        \"\"\"Run complete collection cycle with detailed reporting\"\"\"\n",
        "        cycle_start = time.time()\n",
        "        logging.info(\"=\" * 60)\n",
        "        logging.info(\"STARTING ENHANCED COLLECTION CYCLE\")\n",
        "        logging.info(\"=\" * 60)\n",
        "\n",
        "        results = {\n",
        "            'cycle_start': datetime.now(),\n",
        "            'symbols_processed': [],\n",
        "            'total_records_found': 0,\n",
        "            'total_records_saved': 0,\n",
        "            'successful_symbols': [],\n",
        "            'failed_symbols': [],\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        for symbol in self.symbols:\n",
        "            try:\n",
        "                symbol_result = await self.collect_data_for_symbol(symbol)\n",
        "                results['symbols_processed'].append(symbol_result)\n",
        "                results['total_records_found'] += symbol_result['records_found']\n",
        "                results['total_records_saved'] += symbol_result['records_saved']\n",
        "\n",
        "                if symbol_result['success']:\n",
        "                    results['successful_symbols'].append(symbol)\n",
        "                else:\n",
        "                    results['failed_symbols'].append(symbol)\n",
        "                    if 'error' in symbol_result:\n",
        "                        results['errors'].append(f\"{symbol}: {symbol_result['error']}\")\n",
        "\n",
        "                # Small delay between symbols\n",
        "                await asyncio.sleep(3)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Failed to process {symbol}: {e}\"\n",
        "                logging.error(error_msg)\n",
        "                results['errors'].append(error_msg)\n",
        "                results['failed_symbols'].append(symbol)\n",
        "\n",
        "        cycle_time = time.time() - cycle_start\n",
        "\n",
        "        # Log comprehensive results\n",
        "        logging.info(\"=\" * 60)\n",
        "        logging.info(\"COLLECTION CYCLE COMPLETE\")\n",
        "        logging.info(f\"Duration: {cycle_time:.1f} seconds\")\n",
        "        logging.info(f\"Records Found: {results['total_records_found']}\")\n",
        "        logging.info(f\"Records Saved: {results['total_records_saved']}\")\n",
        "        logging.info(f\"Successful Symbols: {len(results['successful_symbols'])}/{len(self.symbols)}\")\n",
        "\n",
        "        if results['successful_symbols']:\n",
        "            logging.info(f\"Success: {', '.join(results['successful_symbols'])}\")\n",
        "        if results['failed_symbols']:\n",
        "            logging.warning(f\"Failed: {', '.join(results['failed_symbols'])}\")\n",
        "        if results['errors']:\n",
        "            logging.error(f\"Errors encountered: {len(results['errors'])}\")\n",
        "\n",
        "        logging.info(\"=\" * 60)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def calculate_aggregated_scores(self) -> Dict:\n",
        "        \"\"\"Calculate and save aggregated sentiment scores\"\"\"\n",
        "        logging.info(\"Calculating aggregated sentiment scores...\")\n",
        "\n",
        "        aggregation_results = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'symbols_processed': [],\n",
        "            'summary': {}\n",
        "        }\n",
        "\n",
        "        for symbol in self.symbols:\n",
        "            try:\n",
        "                # Get recent data from database\n",
        "                conn = sqlite3.connect(self.db_manager.db_path)\n",
        "\n",
        "                query = '''\n",
        "                    SELECT * FROM sentiment_data\n",
        "                    WHERE symbol = ? AND timestamp > datetime('now', '-3 hours')\n",
        "                    ORDER BY timestamp DESC\n",
        "                '''\n",
        "\n",
        "                df = pd.read_sql_query(query, conn, params=(symbol,))\n",
        "                conn.close()\n",
        "\n",
        "                if not df.empty:\n",
        "                    # Calculate metrics\n",
        "                    total_mentions = len(df)\n",
        "                    positive_count = len(df[df['sentiment_score'] > 0.5])\n",
        "                    negative_count = len(df[df['sentiment_score'] < 0.5])\n",
        "                    neutral_count = len(df[df['sentiment_score'] == 0.5])\n",
        "\n",
        "                    average_sentiment = df['sentiment_score'].mean()\n",
        "                    confidence_weighted_score = np.average(df['sentiment_score'], weights=df['confidence'])\n",
        "                    volatility_score = df['sentiment_score'].std() if len(df) > 1 else 0.0\n",
        "\n",
        "                    # Extract trending topics\n",
        "                    trending_topics = self.extract_trending_topics(df['content'].tolist())\n",
        "\n",
        "                    # Save to aggregated_scores table\n",
        "                    self.save_aggregated_score(symbol, {\n",
        "                        'average_sentiment': average_sentiment,\n",
        "                        'positive_count': positive_count,\n",
        "                        'negative_count': negative_count,\n",
        "                        'neutral_count': neutral_count,\n",
        "                        'total_mentions': total_mentions,\n",
        "                        'confidence_weighted_score': confidence_weighted_score,\n",
        "                        'volatility_score': volatility_score,\n",
        "                        'trending_topics': trending_topics,\n",
        "                        'sector_sentiment': average_sentiment,\n",
        "                        'market_momentum': 0.5,\n",
        "                        'regulatory_impact': 0.5,\n",
        "                        'economic_context': 0.5,\n",
        "                        'prediction_confidence': min(total_mentions / 10, 1.0)\n",
        "                    })\n",
        "\n",
        "                    aggregation_results['symbols_processed'].append(symbol)\n",
        "\n",
        "                    # Log symbol results\n",
        "                    status = \"POSITIVE\" if confidence_weighted_score < 0.5 else \"NEGATIVE\" if confidence_weighted_score > 0.5 else \"NEUTRAL\"\n",
        "                    company_info = self.db_manager.get_company_info(symbol)\n",
        "                    logging.info(f\"{symbol} ({company_info['company_name']}): {status} | Score: {confidence_weighted_score:.3f} | Mentions: {total_mentions}\")\n",
        "\n",
        "                else:\n",
        "                    # No data available - save default metrics\n",
        "                    self.save_aggregated_score(symbol, {\n",
        "                        'average_sentiment': 0.5,\n",
        "                        'positive_count': 0,\n",
        "                        'negative_count': 0,\n",
        "                        'neutral_count': 0,\n",
        "                        'total_mentions': 0,\n",
        "                        'confidence_weighted_score': 0.5,\n",
        "                        'volatility_score': 0.0,\n",
        "                        'trending_topics': [],\n",
        "                        'sector_sentiment': 0.5,\n",
        "                        'market_momentum': 0.5,\n",
        "                        'regulatory_impact': 0.5,\n",
        "                        'economic_context': 0.5,\n",
        "                        'prediction_confidence': 0.0\n",
        "                    })\n",
        "\n",
        "                    logging.info(f\"{symbol}: NO DATA - Using default neutral sentiment\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error calculating aggregation for {symbol}: {e}\")\n",
        "\n",
        "        return aggregation_results\n",
        "\n",
        "    def extract_trending_topics(self, texts: List[str]) -> List[str]:\n",
        "        \"\"\"Extract trending topics from text content\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            combined_text = ' '.join(texts)\n",
        "\n",
        "            # Simple keyword extraction\n",
        "            words = re.findall(r'\\b[a-zA-Z]{4,}\\b', combined_text.lower())\n",
        "\n",
        "            # Remove common words\n",
        "            stopwords = {'that', 'this', 'with', 'from', 'they', 'been', 'have', 'were', 'said', 'each', 'which', 'their', 'time', 'will'}\n",
        "            filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "            # Get most common words\n",
        "            word_counts = Counter(filtered_words)\n",
        "            trending = [word for word, count in word_counts.most_common(5) if count > 1]\n",
        "\n",
        "            return trending\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error extracting trending topics: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_aggregated_score(self, symbol: str, metrics: Dict):\n",
        "        \"\"\"Save aggregated score to database\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_manager.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            company_info = self.db_manager.get_company_info(symbol)\n",
        "            now = datetime.now()\n",
        "            period_start = now - timedelta(hours=3)\n",
        "\n",
        "            cursor.execute('''\n",
        "                INSERT INTO aggregated_scores\n",
        "                (timestamp, symbol, company_name, period_start, period_end, average_sentiment,\n",
        "                 positive_count, negative_count, neutral_count, total_mentions,\n",
        "                 confidence_weighted_score, volatility_score, trending_topics, sector_sentiment,\n",
        "                 market_momentum, regulatory_impact, economic_context, prediction_confidence)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                now, symbol, company_info['company_name'], period_start, now,\n",
        "                metrics['average_sentiment'], metrics['positive_count'],\n",
        "                metrics['negative_count'], metrics['neutral_count'], metrics['total_mentions'],\n",
        "                metrics['confidence_weighted_score'], metrics['volatility_score'],\n",
        "                json.dumps(metrics['trending_topics']), metrics['sector_sentiment'],\n",
        "                metrics['market_momentum'], metrics['regulatory_impact'],\n",
        "                metrics['economic_context'], metrics['prediction_confidence']\n",
        "            ))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving aggregated score for {symbol}: {e}\")\n",
        "\n",
        "    def get_current_sentiment_summary(self, symbol: str) -> Dict:\n",
        "        \"\"\"Get current sentiment summary for a symbol\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_manager.db_path)\n",
        "\n",
        "            # Get latest aggregated score\n",
        "            query = '''\n",
        "                SELECT * FROM aggregated_scores\n",
        "                WHERE symbol = ?\n",
        "                ORDER BY timestamp DESC\n",
        "                LIMIT 1\n",
        "            '''\n",
        "\n",
        "            df = pd.read_sql_query(query, conn, params=(symbol,))\n",
        "\n",
        "            if df.empty:\n",
        "                company_info = self.db_manager.get_company_info(symbol)\n",
        "                return {\n",
        "                    'symbol': symbol,\n",
        "                    'company_name': company_info['company_name'],\n",
        "                    'current_sentiment_score': 0.5,\n",
        "                    'sentiment_status': 'NEUTRAL',\n",
        "                    'total_mentions': 0,\n",
        "                    'prediction_confidence': 0.0,\n",
        "                    'trend_direction': 'STABLE',\n",
        "                    'last_updated': 'Never'\n",
        "                }\n",
        "\n",
        "            latest_data = df.iloc[0].to_dict()\n",
        "\n",
        "            # Determine sentiment status\n",
        "            score = latest_data.get('confidence_weighted_score', 0.5)\n",
        "            if score > 0.7:\n",
        "                status = \"VERY POSITIVE\"\n",
        "            elif score >0.5 and score <= 0.7:\n",
        "                status = \"POSITIVE\"\n",
        "            elif score == 0.5:\n",
        "                status = \"NEUTRAL\"\n",
        "            elif score >-0.5 and score < 0.5:\n",
        "                status = \"NEGATIVE\"\n",
        "            else:\n",
        "                status = \"VERY NEGATIVE\"\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "            return {\n",
        "                'symbol': symbol,\n",
        "                'company_name': latest_data.get('company_name', symbol),\n",
        "                'current_sentiment_score': score,\n",
        "                'sentiment_status': status,\n",
        "                'total_mentions': latest_data.get('total_mentions', 0),\n",
        "                'prediction_confidence': latest_data.get('prediction_confidence', 0.0),\n",
        "                'trend_direction': 'STABLE',  # Simplified for now\n",
        "                'last_updated': latest_data.get('timestamp', 'Unknown')\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error getting sentiment summary for {symbol}: {e}\")\n",
        "            company_info = self.db_manager.get_company_info(symbol)\n",
        "            return {\n",
        "                'symbol': symbol,\n",
        "                'company_name': company_info['company_name'],\n",
        "                'current_sentiment_score': 0.5,\n",
        "                'sentiment_status': 'ERROR',\n",
        "                'total_mentions': 0,\n",
        "                'prediction_confidence': 0.0,\n",
        "                'trend_direction': 'UNKNOWN',\n",
        "                'last_updated': 'Error'\n",
        "            }\n",
        "\n",
        "    def get_scraping_diagnostics(self) -> Dict:\n",
        "        \"\"\"Get scraping diagnostics for troubleshooting\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_manager.db_path)\n",
        "\n",
        "            # Get recent scraping logs\n",
        "            query = '''\n",
        "                SELECT source, symbol, success, records_found, error_message, response_time\n",
        "                FROM scraping_logs\n",
        "                WHERE timestamp > datetime('now', '-24 hours')\n",
        "                ORDER BY timestamp DESC\n",
        "            '''\n",
        "\n",
        "            df = pd.read_sql_query(query, conn)\n",
        "            conn.close()\n",
        "\n",
        "            if df.empty:\n",
        "                return {'status': 'No scraping logs found'}\n",
        "\n",
        "            # Calculate diagnostics\n",
        "            diagnostics = {\n",
        "                'total_attempts': len(df),\n",
        "                'successful_attempts': len(df[df['success'] == True]),\n",
        "                'failed_attempts': len(df[df['success'] == False]),\n",
        "                'success_rate': len(df[df['success'] == True]) / len(df) * 100,\n",
        "                'total_records_found': df['records_found'].sum(),\n",
        "                'average_response_time': df['response_time'].mean(),\n",
        "                'sources_attempted': df['source'].nunique(),\n",
        "                'symbols_attempted': df['symbol'].nunique(),\n",
        "                'recent_errors': df[df['success'] == False]['error_message'].value_counts().to_dict()\n",
        "            }\n",
        "\n",
        "            return diagnostics\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error getting scraping diagnostics: {e}\")\n",
        "            return {'status': 'Error retrieving diagnostics', 'error': str(e)}\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        if self.web_scraper:\n",
        "            self.web_scraper.close_selenium()\n",
        "\n",
        "async def run_fixed_demo():\n",
        "    \"\"\"Run the fixed demo with enhanced debugging\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\" KENYAN STOCK MARKET SENTIMENT ANALYSIS SYSTEM\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Enhanced with improved web scraping and comprehensive error handling\")\n",
        "    print()\n",
        "\n",
        "    # Test with fewer symbols initially\n",
        "    test_symbols = ['SCOM', 'EQTY', 'KCB', 'EABL']\n",
        "\n",
        "    print(f\"Testing with symbols: {', '.join(test_symbols)}\")\n",
        "    print(\"Sentiment Scale: 0.5-1.0 = POSITIVE, -0.5-0.5 = NEGATIVE\")\n",
        "    print()\n",
        "\n",
        "    # Initialize system\n",
        "    system = KenyanSentimentSystem(symbols=test_symbols)\n",
        "\n",
        "    try:\n",
        "        print(\"1. SYSTEM DIAGNOSTICS\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"✓ Database initialized: {system.db_manager.db_path}\")\n",
        "        print(f\"✓ Sentiment analyzer ready: {system.sentiment_analyzer.sia is not None}\")\n",
        "        print(f\"✓ Selenium available: {system.web_scraper.selenium_available}\")\n",
        "        print(f\"✓ Requests session ready: {system.web_scraper.session is not None}\")\n",
        "        print()\n",
        "\n",
        "        print(\"2. RUNNING ENHANCED DATA COLLECTION...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Run collection cycle\n",
        "        collection_results = await system.run_collection_cycle()\n",
        "\n",
        "        print(f\"✓ Symbols processed: {len(collection_results['symbols_processed'])}\")\n",
        "        print(f\"✓ Total records found: {collection_results['total_records_found']}\")\n",
        "        print(f\"✓ Total records saved: {collection_results['total_records_saved']}\")\n",
        "        print(f\"✓ Success rate: {len(collection_results['successful_symbols'])}/{len(test_symbols)} symbols\")\n",
        "\n",
        "        if collection_results['successful_symbols']:\n",
        "            print(f\"✓ Successful: {', '.join(collection_results['successful_symbols'])}\")\n",
        "        if collection_results['failed_symbols']:\n",
        "            print(f\"⚠ Failed: {', '.join(collection_results['failed_symbols'])}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"3. CALCULATING SENTIMENT AGGREGATIONS...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Calculate aggregations\n",
        "        aggregation_results = system.calculate_aggregated_scores()\n",
        "\n",
        "        print(f\"✓ Symbols with aggregations: {len(aggregation_results['symbols_processed'])}\")\n",
        "        print()\n",
        "\n",
        "        print(\"4. CURRENT SENTIMENT RESULTS\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"{'Symbol':<6} | {'Company':<25} | {'Status':<13} | {'Score':<7} | {'Mentions':<8} | {'Confidence':<10}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Display results\n",
        "        for symbol in test_symbols:\n",
        "            summary = system.get_current_sentiment_summary(symbol)\n",
        "\n",
        "            company_name = summary['company_name'][:25]\n",
        "            status = summary['sentiment_status']\n",
        "            score = summary['current_sentiment_score']\n",
        "            mentions = summary['total_mentions']\n",
        "            confidence = summary['prediction_confidence']\n",
        "\n",
        "            print(f\"{symbol:<6} | {company_name:<25} | {status:<13} | {score:<7.3f} | {mentions:<8} | {confidence:<10.2f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"5. SCRAPING DIAGNOSTICS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        diagnostics = system.get_scraping_diagnostics()\n",
        "\n",
        "        if 'total_attempts' in diagnostics:\n",
        "            print(f\"✓ Total scraping attempts: {diagnostics['total_attempts']}\")\n",
        "            print(f\"✓ Successful attempts: {diagnostics['successful_attempts']}\")\n",
        "            print(f\"✓ Success rate: {diagnostics['success_rate']:.1f}%\")\n",
        "            print(f\"✓ Total records found: {diagnostics['total_records_found']}\")\n",
        "            print(f\"✓ Average response time: {diagnostics['average_response_time']:.2f}s\")\n",
        "            print(f\"✓ Sources attempted: {diagnostics['sources_attempted']}\")\n",
        "\n",
        "            if diagnostics['recent_errors']:\n",
        "                print(\"\\nRecent errors:\")\n",
        "                for error, count in list(diagnostics['recent_errors'].items())[:3]:\n",
        "                    print(f\"  - {error}: {count} times\")\n",
        "        else:\n",
        "            print(\"No diagnostics available yet\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"6. TROUBLESHOOTING TIPS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if collection_results['total_records_found'] == 0:\n",
        "            print(\"⚠ No records found. Possible issues:\")\n",
        "            print(\"  - Websites may have changed their structure\")\n",
        "            print(\"  - Network connectivity issues\")\n",
        "            print(\"  - Anti-scraping measures blocking requests\")\n",
        "            print(\"  - Content may not contain the target keywords\")\n",
        "            print()\n",
        "            print(\"✓ Solutions:\")\n",
        "            print(\"  - Check internet connection\")\n",
        "            print(\"  - Update Chrome/ChromeDriver\")\n",
        "            print(\"  - Try running again (some sites have rate limiting)\")\n",
        "            print(\"  - Check the scraping logs in the database\")\n",
        "        else:\n",
        "            print(\"✓ Data collection working!\")\n",
        "            print(f\"  - Successfully collected {collection_results['total_records_found']} records\")\n",
        "            print(f\"  - {len(collection_results['successful_symbols'])} symbols processed successfully\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"7. NEXT \")\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✓ System is ready for continuous monitoring\")\n",
        "        print(\"✓ Database contains structured sentiment data\")\n",
        "        print(\"✓ Data format optimized for predictive algorithms\")\n",
        "        print()\n",
        "        print(\"Manual commands:\")\n",
        "        print(\"• Get diagnostics: system.get_scraping_diagnostics()\")\n",
        "        print(\"• Run collection: await system.run_collection_cycle()\")\n",
        "        print(\"• Calculate scores: system.calculate_aggregated_scores()\")\n",
        "        print(\"• Get summary: system.get_current_sentiment_summary('SCOM')\")\n",
        "\n",
        "        print()\n",
        "        print(\"=\" * 80)\n",
        "        print(\"FIXED SYSTEM READY FOR KENYAN MARKET ANALYSIS!\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return system\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Demo error: {e}\")\n",
        "        print(f\"❌ Demo error: {e}\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        system.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the fixed demo\n",
        "    asyncio.run(run_fixed_demo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gFN2ZpBls7m",
        "outputId": "fb2f506d-ebed-453b-b628-90136e24fa2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
            "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "psMYPHIymRn6"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIyZFbZLT3U5",
        "outputId": "220379b5-2137-43ae-ccfb-e84200047444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install certifi"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN37+uuPpaw+GXdfkAt+k0D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}